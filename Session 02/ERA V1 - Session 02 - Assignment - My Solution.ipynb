{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJptKBxALl-u",
        "outputId": "eb001d51-3fcf-41da-a838-a74386a1725f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "# Install torchsummary\n",
        "!pip install torchsummary\n",
        "\n",
        "# Import libraries and methods\n",
        "from torchsummary import summary\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Owi1LBNY8L",
        "outputId": "af655f4b-f78f-4f06-825e-851484f4d492"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Check if GPU/ Cuda is available and set the device accordingly\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "# Print the final device being used\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQZaZRGcNLtr",
        "outputId": "31aa23c4-afc8-4630-999b-ec5652c88cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 210868674.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 14361078.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 71610870.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 20550732.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the batch size\n",
        "# The batch size defines the number of samples that will be propagated through the network\n",
        "# The ideal way to set batch size is to monitor resource utilization (CPU, GPU, RAM) during training\n",
        "batch_size = 128\n",
        "\n",
        "# Define the loaders\n",
        "# The loaders are used to load the data in batches\n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    # Load the MNIST dataset\n",
        "    datasets.MNIST(\n",
        "        \"../data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        #    Define the transformations to be applied on the images\n",
        "        transform=transforms.Compose(\n",
        "            [\n",
        "                # Convert the images to tensors\n",
        "                transforms.ToTensor(),\n",
        "                # Normalize the images with mean and standard deviation for each channel\n",
        "                transforms.Normalize((0.1307,), (0.3081,)),\n",
        "            ]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Define the test loader in a similar way to the train loader\n",
        "# Remember to set train to False\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        \"../data\",\n",
        "        train=False,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3gEjf-xMb-N"
      },
      "source": [
        "# Some Notes on our naive model\n",
        "\n",
        "We are going to write a network based on what we have learnt so far.\n",
        "\n",
        "The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sir2LmSVLr_4"
      },
      "outputs": [],
      "source": [
        "# Create a class to create the network\n",
        "# All the convolutions used in the network are 3x3 kernels\n",
        "class FirstDNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        # Call the constructor of the parent class: nn.Module\n",
        "        super(FirstDNN, self).__init__()\n",
        "        # Define the convolution layers in the network\n",
        "\n",
        "        # r_in - Input receptive field\n",
        "        # n_in - Number of input features\n",
        "        # j_in - Input jump or representation power of pixels, initializes with 1 on first layer\n",
        "        # s - stride\n",
        "        # r_out - Output receptive field, calculated as (r_in + (k-1)*j_in)\n",
        "        # n_out - Number of output features, calculated as (((n_in + 2*p - k)/s) + 1)\n",
        "        # j_out - Output jump or representation power of pixels, calculated as j_in * s\n",
        "\n",
        "        # First layer: convolution\n",
        "        # r_in:1, n_in:28, j_in:1, s:1, r_out:3, n_out:28, j_out:1\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "\n",
        "        # Second layer: convolution\n",
        "        # r_in:3 , n_in:28 , j_in:1 , s:1 , r_out:5 , n_out:28 , j_out:1\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "\n",
        "        # Third layer: max pooling\n",
        "        # r_in:5 , n_in:28 , j_in:1 , s:2 , r_out:6 , n_out:14 , j_out:2\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Fourth layer: convolution\n",
        "        # r_in:6 , n_in:14 , j_in:2 , s:1 , r_out:10 , n_out:14 , j_out:2\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "\n",
        "        # Fifth layer: convolution\n",
        "        # r_in:10 , n_in:14 , j_in:2 , s:1 , r_out:14 , n_out:14 , j_out:2\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "\n",
        "        # Sixth layer: max pooling\n",
        "        # r_in:14 , n_in:14 , j_in:2 , s:2 , r_out:16 , n_out:7 , j_out:4\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Seventh layer: convolution\n",
        "        # r_in:16 , n_in:7 , j_in:4 , s:1 , r_out:24 , n_out:5 , j_out:4\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "\n",
        "        # Eighth layer: convolution\n",
        "        # r_in:24 , n_in:5 , j_in:4 , s:1 , r_out:32 , n_out:3 , j_out:4\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "\n",
        "        # Ninth layer: convolution\n",
        "        # r_in:32 , n_in:3 , j_in:4 , s:1 , r_out:40 , n_out:1 , j_out:4\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "\n",
        "    # Define the forward pass\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the first convolution layer, followed by a relu activation, followed by the second convolution layer, followed by a relu activation, followed by the max pooling layer\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "        # Pass the input through the third convolution layer, followed by a relu activation, followed by the fourth convolution layer, followed by a relu activation, followed by the max pooling layer\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        # Pass the input through the fifth convolution layer, followed by a relu activation, followed by the sixth convolution layer, followed by a relu activation\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        # Pass the input through the seventh convolution layer, followed by a relu activation\n",
        "        x = F.relu(self.conv7(x))\n",
        "        # Flatten the output of the seventh convolution layer\n",
        "        x = x.view(-1, 10)\n",
        "        # Return the output by applying a softmax activation with log probabilities\n",
        "        return F.log_softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxICO4TTNt2H"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the network and send it to the device\n",
        "# https://pytorch.org/docs/stable/generated/torch.Tensor.to.html\n",
        "model = FirstDNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M__MtFIYNwXa",
        "outputId": "1872a6c8-738f-4cec-8f21-e38666355954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-0e0792b7ac56>:66: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ],
      "source": [
        "# Print the model summary by specifying the input size\n",
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_vlC-bdNzo1"
      },
      "outputs": [],
      "source": [
        "# Defining functions to train and test the network\n",
        "\n",
        "\n",
        "# Function to train the network given the model, device, train loader, optimizer and epoch\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # Set the model to training mode\n",
        "    # Depending on the mode, the model behaves differently. Some steps are used only during training, such as calculating the gradients and updating the weights\n",
        "    model.train()\n",
        "    # tqdm is used to display the progress message\n",
        "    pbar = tqdm(train_loader)\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        # Send the input and target to the device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Clear the gradients calculated from the last iteration\n",
        "        # https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\n",
        "        optimizer.zero_grad()\n",
        "        # Pass the input through the model\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate the negative log likelihood loss for the batch by comparing the model's output to the target\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # The backward() method is used to compute the gradients of the loss with respect to the parameters of the model\n",
        "        loss.backward()\n",
        "\n",
        "        # The step() method is used to update the parameters of the model in the direction that minimizes the loss using the gradients computed in the backward() method\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the progress bar with the loss for the batch\n",
        "        pbar.set_description(desc=f\"loss={loss.item()} batch_id={batch_idx}\")\n",
        "\n",
        "\n",
        "# Function to test the network given the model, device and test loader\n",
        "def test(model, device, test_loader):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Define variables to keep track of the test loss and the number of correct predictions\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # Tell PyTorch not to calculate gradients by using the no_grad() context manager\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the test data\n",
        "        for data, target in test_loader:\n",
        "            # Send the input and target to the device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            # Pass the input through the model\n",
        "            output = model(data)\n",
        "\n",
        "            # Calculate the negative log likelihood loss for the batch by comparing the model's output to the target\n",
        "            # Sum up batch loss\n",
        "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
        "\n",
        "            # Get the index of the max log-probability\n",
        "            # Out of the 10 output values, find the index of the one with the highest value\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "            # If the prediction is correct by comparing to target with same dimension, increment the correct counter\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # Divide the test loss by the number of examples in the test set to get the average loss\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    # Print the average loss and the accuracy for the test set\n",
        "    print(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            len(test_loader.dataset),\n",
        "            100.0 * correct / len(test_loader.dataset),\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0FYVWkGOFBS",
        "outputId": "d31986f0-d4f0-4526-f786-29c4d66c9f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-4-0e0792b7ac56>:66: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=0.8639147281646729 batch_id=468: 100%|██████████| 469/469 [00:35<00:00, 13.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.9417, Accuracy: 6066/10000 (61%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of the stochastic gradient descent optimizer\n",
        "# https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
        "# lr is the learning rate which controls the step size of the optimizer\n",
        "# momentum is a parameter that controls the amount of inertia in the optimizer i.e. how much the previous step affects the current step\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Running for 1 epoch\n",
        "for epoch in range(1, 2):\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reIBU667OG_c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}