{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EH7n8CYIt3yt",
        "ahIZYziluEHS",
        "9ECH9vN_umz9",
        "CLaHTKtQu72T"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning from Scratch: Stable Diffusion\n",
        "Binxu Wang\n",
        "\n",
        "Nov.2022\n",
        "\n",
        "This notebook walk you through how to build your Unet architecture from scratch! All the network components are defined in a single notebook."
      ],
      "metadata": {
        "id": "rNwRr-frtUWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "habUA6HPrWzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "271Up9v8q2pp"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from collections import OrderedDict\n",
        "from easydict import EasyDict as edict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define our Unet Architecture"
      ],
      "metadata": {
        "id": "xiDr0zU3tAbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Our ResBlock"
      ],
      "metadata": {
        "id": "x_n42yfVsDKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# backbone, Residual Block (Checked)\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel, time_emb_dim, out_channel=None, ):\n",
        "        super().__init__()\n",
        "        if out_channel is None:\n",
        "            out_channel = in_channel\n",
        "        self.norm1 = nn.GroupNorm(32, in_channel, eps=1e-05, affine=True)\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=1, padding=1)\n",
        "        self.time_emb_proj = nn.Linear(in_features=time_emb_dim, out_features=out_channel, bias=True)\n",
        "        self.norm2 = nn.GroupNorm(32, out_channel, eps=1e-05, affine=True)\n",
        "        self.dropout = nn.Dropout(p=0.0, inplace=False)\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=1, padding=1)\n",
        "        self.nonlinearity = nn.SiLU()\n",
        "        if in_channel == out_channel:\n",
        "            self.conv_shortcut = nn.Identity()\n",
        "        else:\n",
        "            self.conv_shortcut = nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x, t_emb, cond=None):\n",
        "        # Input conv\n",
        "        h = self.norm1(x)\n",
        "        h = self.nonlinearity(h)\n",
        "        h = self.conv1(h)\n",
        "        # Time modulation\n",
        "        if t_emb is not None:\n",
        "            t_hidden = self.time_emb_proj(self.nonlinearity(t_emb))\n",
        "            h = h + t_hidden[:, :, None, None]\n",
        "        # Output conv\n",
        "        h = self.norm2(h)\n",
        "        h = self.nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "        # Skip connection\n",
        "        return h + self.conv_shortcut(x)\n",
        "\n",
        "\n",
        "# UpSampling (Checked)\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, channel, scale_factor=2, mode='nearest'):\n",
        "        super(UpSample, self).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "        self.mode = mode\n",
        "        self.conv = nn.Conv2d(channel, channel, kernel_size=3, stride=1, padding=1, )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "# DownSampling (Checked)\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self, channel, ):\n",
        "        super(DownSample, self).__init__()\n",
        "        self.conv = nn.Conv2d(channel, channel, kernel_size=3, stride=2, padding=1, )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)  # F.interpolate(x, scale_factor=1/self.scale_factor, mode=self.mode)"
      ],
      "metadata": {
        "id": "RSB8S64Jrb9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Our Attention / Transformer"
      ],
      "metadata": {
        "id": "9zxxh3AFsGMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Self and Cross Attention mechanism (Checked)\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"General implementation of Cross & Self Attention multi-head\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, hidden_dim, context_dim=None, num_heads=8, ):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.context_dim = context_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.to_q = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
        "        if context_dim is None:\n",
        "            # Self Attention\n",
        "            self.to_k = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
        "            self.to_v = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
        "            self.self_attn = True\n",
        "        else:\n",
        "            # Cross Attention\n",
        "            self.to_k = nn.Linear(context_dim, embed_dim, bias=False)\n",
        "            self.to_v = nn.Linear(context_dim, embed_dim, bias=False)\n",
        "            self.self_attn = False\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim, bias=True)\n",
        "        )  # this could be omitted\n",
        "\n",
        "    def forward(self, tokens, context=None):\n",
        "        Q = self.to_q(tokens)\n",
        "        K = self.to_k(tokens) if self.self_attn else self.to_k(context)\n",
        "        V = self.to_v(tokens) if self.self_attn else self.to_v(context)\n",
        "        # print(Q.shape, K.shape, V.shape)\n",
        "        # transform heads onto batch dimension\n",
        "        Q = rearrange(Q, 'B T (H D) -> (B H) T D', H=self.num_heads, D=self.head_dim)\n",
        "        K = rearrange(K, 'B T (H D) -> (B H) T D', H=self.num_heads, D=self.head_dim)\n",
        "        V = rearrange(V, 'B T (H D) -> (B H) T D', H=self.num_heads, D=self.head_dim)\n",
        "        # print(Q.shape, K.shape, V.shape)\n",
        "        scoremats = torch.einsum(\"BTD,BSD->BTS\", Q, K)\n",
        "        attnmats = F.softmax(scoremats / math.sqrt(self.head_dim), dim=-1)\n",
        "        # print(scoremats.shape, attnmats.shape, )\n",
        "        ctx_vecs = torch.einsum(\"BTS,BSD->BTD\", attnmats, V)\n",
        "        # split the heads transform back to hidden.\n",
        "        ctx_vecs = rearrange(ctx_vecs, '(B H) T D -> B T (H D)', H=self.num_heads, D=self.head_dim)\n",
        "        # TODO: note this `to_out` is also a linear layer, could be in principle merged into the to_value layer.\n",
        "        return self.to_out(ctx_vecs)\n"
      ],
      "metadata": {
        "id": "PhTnJAFmsM6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer layers\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim, context_dim, num_heads=8):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attn1 = CrossAttention(hidden_dim, hidden_dim, num_heads=num_heads)  # self attention\n",
        "        self.attn2 = CrossAttention(hidden_dim, hidden_dim, context_dim, num_heads=num_heads)  # cross attention\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
        "        # to be compatible with Diffuser, could simplify.\n",
        "        self.ff = FeedForward_GEGLU(hidden_dim, )\n",
        "        # A more common version used in transformers.\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     nn.Linear(hidden_dim, 3 * hidden_dim),\n",
        "        #     nn.GELU(),\n",
        "        #     nn.Linear(3 * hidden_dim, hidden_dim)\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        x = self.attn1(self.norm1(x)) + x\n",
        "        x = self.attn2(self.norm2(x), context=context) + x\n",
        "        x = self.ff(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class GEGLU_proj(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(GEGLU_proj, self).__init__()\n",
        "        self.proj = nn.Linear(in_dim, 2 * out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x, gates = x.chunk(2, dim=-1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "\n",
        "class FeedForward_GEGLU(nn.Module):\n",
        "    # https://github.com/huggingface/diffusers/blob/95414bd6bf9bb34a312a7c55f10ba9b379f33890/src/diffusers/models/attention.py#L339\n",
        "    # A variant of the gated linear unit activation function from https://arxiv.org/abs/2002.05202.\n",
        "    def __init__(self, hidden_dim, mult=4):\n",
        "        super(FeedForward_GEGLU, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            GEGLU_proj(hidden_dim, mult * hidden_dim),\n",
        "            nn.Dropout(0.0),\n",
        "            nn.Linear(mult * hidden_dim, hidden_dim)\n",
        "        )  # to be compatible with Diffuser, could simplify.\n",
        "\n",
        "    def forward(self, x, ):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, hidden_dim, context_dim, num_heads=8):\n",
        "        super(SpatialTransformer, self).__init__()\n",
        "        self.norm = nn.GroupNorm(32, hidden_dim, eps=1e-6, affine=True)\n",
        "        self.proj_in = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=1, stride=1, padding=0)\n",
        "        # self.transformer = TransformerBlock(hidden_dim, context_dim, num_heads=8)\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            TransformerBlock(hidden_dim, context_dim, num_heads=8)\n",
        "        )  # to be compatible with Diffuser, could simplify.\n",
        "        self.proj_out = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, cond=None):\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        # context = rearrange(context, \"b c T -> b T c\")\n",
        "        x = self.proj_in(self.norm(x))\n",
        "        x = rearrange(x, \"b c h w->b (h w) c\")\n",
        "        x = self.transformer_blocks[0](x, cond)\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
        "        return self.proj_out(x) + x_in\n"
      ],
      "metadata": {
        "id": "bJtqJDnFro1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Container of ResBlock and Spatial Transformers"
      ],
      "metadata": {
        "id": "ufUpHrD-viBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified Container. Modify the nn.Sequential to time modulated Sequential\n",
        "class TimeModulatedSequential(nn.Sequential):\n",
        "    \"\"\" Modify the nn.Sequential to time modulated Sequential \"\"\"\n",
        "    def forward(self, x, t_emb, cond=None):\n",
        "        for module in self:\n",
        "            if isinstance(module, TimeModulatedSequential):\n",
        "                x = module(x, t_emb, cond)\n",
        "            elif isinstance(module, ResBlock):\n",
        "                # For certain layers, add the time modulation.\n",
        "                x = module(x, t_emb)\n",
        "            elif isinstance(module, SpatialTransformer):\n",
        "                # For certain layers, add the class conditioning.\n",
        "                x = module(x, cond=cond)\n",
        "            else:\n",
        "                x = module(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "sWQ4TR6krtpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it Together into UNet!"
      ],
      "metadata": {
        "id": "5JecMwdLr3H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet_SD(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=4,\n",
        "                 base_channels=320,\n",
        "                 time_emb_dim=1280,\n",
        "                 context_dim=768,\n",
        "                 multipliers=(1, 2, 4, 4),\n",
        "                 attn_levels=(0, 1, 2),\n",
        "                 nResAttn_block=2,\n",
        "                 cat_unet=True):\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels\n",
        "        base_channels = base_channels\n",
        "        time_emb_dim = time_emb_dim\n",
        "        context_dim = context_dim\n",
        "        multipliers = multipliers\n",
        "        nlevel = len(multipliers)\n",
        "        self.base_channels = base_channels\n",
        "        # attn_levels = [0, 1, 2]\n",
        "        level_channels = [base_channels * mult for mult in multipliers]\n",
        "        # Transform time into embedding\n",
        "        self.time_embedding = nn.Sequential(OrderedDict({\n",
        "            \"linear_1\": nn.Linear(base_channels, time_emb_dim, bias=True),\n",
        "            \"act\": nn.SiLU(),\n",
        "            \"linear_2\": nn.Linear(time_emb_dim, time_emb_dim, bias=True),\n",
        "        })\n",
        "        )  # 2 layer MLP\n",
        "        self.conv_in = nn.Conv2d(self.in_channels, base_channels, 3, stride=1, padding=1)\n",
        "\n",
        "        # Tensor Downsample blocks\n",
        "        nResAttn_block = nResAttn_block\n",
        "        self.down_blocks = TimeModulatedSequential()  # nn.ModuleList()\n",
        "        self.down_blocks_channels = [base_channels]\n",
        "        cur_chan = base_channels\n",
        "        for i in range(nlevel):\n",
        "            for j in range(nResAttn_block):\n",
        "                res_attn_sandwich = TimeModulatedSequential()\n",
        "                # input_chan of first ResBlock is different from the rest.\n",
        "                res_attn_sandwich.append(ResBlock(in_channel=cur_chan, time_emb_dim=time_emb_dim, out_channel=level_channels[i]))\n",
        "                if i in attn_levels:\n",
        "                    # add attention except for the last level\n",
        "                    res_attn_sandwich.append(SpatialTransformer(level_channels[i], context_dim=context_dim))\n",
        "                cur_chan = level_channels[i]\n",
        "                self.down_blocks.append(res_attn_sandwich)\n",
        "                self.down_blocks_channels.append(cur_chan)\n",
        "            # res_attn_sandwich.append(DownSample(level_channels[i]))\n",
        "            if not i == nlevel - 1:\n",
        "                self.down_blocks.append(TimeModulatedSequential(DownSample(level_channels[i])))\n",
        "                self.down_blocks_channels.append(cur_chan)\n",
        "\n",
        "        self.mid_block = TimeModulatedSequential(\n",
        "            ResBlock(cur_chan, time_emb_dim),\n",
        "            SpatialTransformer(cur_chan, context_dim=context_dim),\n",
        "            ResBlock(cur_chan, time_emb_dim),\n",
        "        )\n",
        "\n",
        "        # Tensor Upsample blocks\n",
        "        self.up_blocks = nn.ModuleList() # TimeModulatedSequential()  #\n",
        "        for i in reversed(range(nlevel)):\n",
        "            for j in range(nResAttn_block + 1):\n",
        "                res_attn_sandwich = TimeModulatedSequential()\n",
        "                res_attn_sandwich.append(ResBlock(in_channel=cur_chan + self.down_blocks_channels.pop(),\n",
        "                                                  time_emb_dim=time_emb_dim, out_channel=level_channels[i]))\n",
        "                if i in attn_levels:\n",
        "                    res_attn_sandwich.append(SpatialTransformer(level_channels[i], context_dim=context_dim))\n",
        "                cur_chan = level_channels[i]\n",
        "                if j == nResAttn_block and i != 0:\n",
        "                    res_attn_sandwich.append(UpSample(level_channels[i]))\n",
        "                self.up_blocks.append(res_attn_sandwich)\n",
        "        # Read out from tensor to latent space\n",
        "        self.output = nn.Sequential(\n",
        "            nn.GroupNorm(32, base_channels, ),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(base_channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "        self.to(self.device)\n",
        "\n",
        "    def time_proj(self, time_steps, max_period: int = 10000):\n",
        "        if time_steps.ndim == 0:\n",
        "            time_steps = time_steps.unsqueeze(0)\n",
        "        half = self.base_channels // 2\n",
        "        frequencies = torch.exp(- math.log(max_period)\n",
        "                                * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "                                ).to(device=time_steps.device)\n",
        "        angles = time_steps[:, None].float() * frequencies[None, :]\n",
        "        return torch.cat([torch.cos(angles), torch.sin(angles)], dim=-1)\n",
        "\n",
        "    def forward(self, x, time_steps, cond=None, encoder_hidden_states=None, output_dict=True):\n",
        "        if cond is None and encoder_hidden_states is not None:\n",
        "            cond = encoder_hidden_states\n",
        "        t_emb = self.time_proj(time_steps)\n",
        "        t_emb = self.time_embedding(t_emb)\n",
        "        x = self.conv_in(x)\n",
        "        down_x_cache = [x]\n",
        "        for module in self.down_blocks:\n",
        "            x = module(x, t_emb, cond)\n",
        "            down_x_cache.append(x)\n",
        "        x = self.mid_block(x, t_emb, cond)\n",
        "        for module in self.up_blocks:\n",
        "            x = module(torch.cat((x, down_x_cache.pop()), dim=1), t_emb, cond)\n",
        "        x = self.output(x)\n",
        "        if output_dict:\n",
        "            return edict(sample=x)\n",
        "        else:\n",
        "            return x\n"
      ],
      "metadata": {
        "id": "G0-WVZ_ir1-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit test the components with the UNet implementation"
      ],
      "metadata": {
        "id": "EH7n8CYIt3yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check ResNet"
      ],
      "metadata": {
        "id": "ahIZYziluEHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_ResBlock(pipe):\n",
        "    tmp_blk = ResBlock(320, 1280).cuda()\n",
        "    std_blk = pipe.unet.down_blocks[0].resnets[0]\n",
        "    SD = std_blk.state_dict()\n",
        "    tmp_blk.load_state_dict(SD)\n",
        "    lat_tmp = torch.randn(3, 320, 32, 32)\n",
        "    temb = torch.randn(3, 1280)\n",
        "    with torch.no_grad():\n",
        "        out = pipe.unet.down_blocks[0].resnets[0](lat_tmp.cuda(),temb.cuda())\n",
        "        out2 = tmp_blk(lat_tmp.cuda(), temb.cuda())\n",
        "\n",
        "    assert torch.allclose(out2, out)\n",
        "\n",
        "test_ResBlock(pipe)"
      ],
      "metadata": {
        "id": "wpj8MbYtuCGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_downsampler(pipe):\n",
        "    tmpdsp = DownSample(320).cuda()\n",
        "    stddsp = pipe.unet.down_blocks[0].downsamplers[0]\n",
        "    SD = stddsp.state_dict()\n",
        "    tmpdsp.load_state_dict(SD)\n",
        "    lat_tmp = torch.randn(3, 320, 32, 32)\n",
        "    with torch.no_grad():\n",
        "        out = stddsp(lat_tmp.cuda())\n",
        "        out2 = tmpdsp(lat_tmp.cuda())\n",
        "\n",
        "    assert torch.allclose(out2, out)\n",
        "\n",
        "def test_upsampler(pipe):\n",
        "    tmpusp = UpSample(1280).cuda()\n",
        "    stdusp = pipe.unet.up_blocks[1].upsamplers[0]\n",
        "    SD = stdusp.state_dict()\n",
        "    tmpusp.load_state_dict(SD)\n",
        "    lat_tmp = torch.randn(3, 1280, 32, 32)\n",
        "    with torch.no_grad():\n",
        "        out = stdusp(lat_tmp.cuda())\n",
        "        out2 = tmpusp(lat_tmp.cuda())\n",
        "\n",
        "    assert torch.allclose(out2, out)\n",
        "\n",
        "test_downsampler(pipe)\n",
        "test_upsampler(pipe)"
      ],
      "metadata": {
        "id": "ODTE3D0quStC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Attention"
      ],
      "metadata": {
        "id": "9ECH9vN_umz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_self_attention(pipe):\n",
        "    tmpSattn = CrossAttention(320, 320, context_dim=None, num_heads=8).cuda()\n",
        "    stdSattn = pipe.unet.down_blocks[0].attentions[0].transformer_blocks[0].attn1\n",
        "    tmpSattn.load_state_dict(stdSattn.state_dict())  # checked\n",
        "    with torch.no_grad():\n",
        "        lat_tmp = torch.randn(3, 32, 320)\n",
        "        out = stdSattn(lat_tmp.cuda())\n",
        "        out2 = tmpSattn(lat_tmp.cuda())\n",
        "    assert torch.allclose(out2, out)  # False\n",
        "\n",
        "#%%\n",
        "# Check Cross attention\n",
        "def test_cross_attention(pipe):\n",
        "    tmpXattn = CrossAttention(320, 320, context_dim=768, num_heads=8).cuda()\n",
        "    stdXattn = pipe.unet.down_blocks[0].attentions[0].transformer_blocks[0].attn2\n",
        "    tmpXattn.load_state_dict(stdXattn.state_dict())  # checked\n",
        "    with torch.no_grad():\n",
        "        lat_tmp = torch.randn(3, 32, 320)\n",
        "        ctx_tmp = torch.randn(3, 5, 768)\n",
        "        out = stdXattn(lat_tmp.cuda(), ctx_tmp.cuda())\n",
        "        out2 = tmpXattn(lat_tmp.cuda(), ctx_tmp.cuda())\n",
        "    assert torch.allclose(out2, out)  # False\n",
        "\n",
        "test_self_attention(pipe)\n",
        "test_cross_attention(pipe)"
      ],
      "metadata": {
        "id": "mBeZ09w_uc_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Transformer"
      ],
      "metadata": {
        "id": "CLaHTKtQu72T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% test TransformerBlock Implementation\n",
        "def test_TransformerBlock(pipe):\n",
        "    tmpTfmer = TransformerBlock(320, context_dim=768, num_heads=8).cuda()\n",
        "    stdTfmer = pipe.unet.down_blocks[0].attentions[0].transformer_blocks[0]\n",
        "    tmpTfmer.load_state_dict(stdTfmer.state_dict())  # checked\n",
        "    with torch.no_grad():\n",
        "        lat_tmp = torch.randn(3, 32, 320)\n",
        "        ctx_tmp = torch.randn(3, 5, 768)\n",
        "        out = tmpTfmer(lat_tmp.cuda(), ctx_tmp.cuda())\n",
        "        out2 = stdTfmer(lat_tmp.cuda(), ctx_tmp.cuda())\n",
        "    assert torch.allclose(out2, out)  # False\n",
        "\n",
        "\n",
        "#%% test SpatialTransformer Implementation\n",
        "def test_SpatialTransformer(pipe):\n",
        "    tmpSpTfmer = SpatialTransformer(320, context_dim=768, num_heads=8).cuda()\n",
        "    stdSpTfmer = pipe.unet.down_blocks[0].attentions[0]\n",
        "    tmpSpTfmer.load_state_dict(stdSpTfmer.state_dict())  # checked\n",
        "    with torch.no_grad():\n",
        "        lat_tmp = torch.randn(3, 320, 8, 8)\n",
        "        ctx_tmp = torch.randn(3, 5, 768)\n",
        "        out = tmpSpTfmer(lat_tmp.cuda(), ctx_tmp.cuda())\n",
        "        out2 = stdSpTfmer(lat_tmp.cuda(), ctx_tmp.cuda())\n",
        "    assert torch.allclose(out2, out)  # False\n",
        "\n",
        "test_TransformerBlock(pipe)\n",
        "test_SpatialTransformer(pipe)"
      ],
      "metadata": {
        "id": "XN_tLHSzu62o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Weights into our UNet!"
      ],
      "metadata": {
        "id": "UDRviSqDsQ5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers tokenizers"
      ],
      "metadata": {
        "id": "Ir7QUBnGs1ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "EoVDMCfvs3gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "H3ovjx71sshL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    use_auth_token=True\n",
        ").to(\"cuda\")\n",
        "def dummy_checker(images, **kwargs): return images, False\n",
        "pipe.safety_checker = dummy_checker"
      ],
      "metadata": {
        "id": "M_GZHkIosvV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Entire UNet model"
      ],
      "metadata": {
        "id": "L5Qb_jNcvWyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utils to load weights\n",
        "def load_pipe_into_our_UNet(myUNet, pipe_unet):\n",
        "    # load the pretrained weights from the pipe into our UNet.\n",
        "    # Loading input and output layers.\n",
        "    myUNet.output[0].load_state_dict(pipe_unet.conv_norm_out.state_dict())\n",
        "    myUNet.output[2].load_state_dict(pipe_unet.conv_out.state_dict())\n",
        "    myUNet.conv_in.load_state_dict(pipe_unet.conv_in.state_dict())\n",
        "    myUNet.time_embedding.load_state_dict(pipe_unet.time_embedding.state_dict())\n",
        "    #% Loading the down blocks\n",
        "    myUNet.down_blocks[0][0].load_state_dict(pipe_unet.down_blocks[0].resnets[0].state_dict())\n",
        "    myUNet.down_blocks[0][1].load_state_dict(pipe_unet.down_blocks[0].attentions[0].state_dict())\n",
        "    myUNet.down_blocks[1][0].load_state_dict(pipe_unet.down_blocks[0].resnets[1].state_dict())\n",
        "    myUNet.down_blocks[1][1].load_state_dict(pipe_unet.down_blocks[0].attentions[1].state_dict())\n",
        "    myUNet.down_blocks[2][0].load_state_dict(pipe_unet.down_blocks[0].downsamplers[0].state_dict())\n",
        "\n",
        "    myUNet.down_blocks[3][0].load_state_dict(pipe_unet.down_blocks[1].resnets[0].state_dict())\n",
        "    myUNet.down_blocks[3][1].load_state_dict(pipe_unet.down_blocks[1].attentions[0].state_dict())\n",
        "    myUNet.down_blocks[4][0].load_state_dict(pipe_unet.down_blocks[1].resnets[1].state_dict())\n",
        "    myUNet.down_blocks[4][1].load_state_dict(pipe_unet.down_blocks[1].attentions[1].state_dict())\n",
        "    myUNet.down_blocks[5][0].load_state_dict(pipe_unet.down_blocks[1].downsamplers[0].state_dict())\n",
        "\n",
        "    myUNet.down_blocks[6][0].load_state_dict(pipe_unet.down_blocks[2].resnets[0].state_dict())\n",
        "    myUNet.down_blocks[6][1].load_state_dict(pipe_unet.down_blocks[2].attentions[0].state_dict())\n",
        "    myUNet.down_blocks[7][0].load_state_dict(pipe_unet.down_blocks[2].resnets[1].state_dict())\n",
        "    myUNet.down_blocks[7][1].load_state_dict(pipe_unet.down_blocks[2].attentions[1].state_dict())\n",
        "    myUNet.down_blocks[8][0].load_state_dict(pipe_unet.down_blocks[2].downsamplers[0].state_dict())\n",
        "\n",
        "    myUNet.down_blocks[9][0].load_state_dict(pipe_unet.down_blocks[3].resnets[0].state_dict())\n",
        "    myUNet.down_blocks[10][0].load_state_dict(pipe_unet.down_blocks[3].resnets[1].state_dict())\n",
        "\n",
        "    #% Loading the middle blocks\n",
        "    myUNet.mid_block[0].load_state_dict(pipe_unet.mid_block.resnets[0].state_dict())\n",
        "    myUNet.mid_block[1].load_state_dict(pipe_unet.mid_block.attentions[0].state_dict())\n",
        "    myUNet.mid_block[2].load_state_dict(pipe_unet.mid_block.resnets[1].state_dict())\n",
        "    # % Loading the up blocks\n",
        "    # upblock 0\n",
        "    myUNet.up_blocks[0][0].load_state_dict(pipe_unet.up_blocks[0].resnets[0].state_dict())\n",
        "    myUNet.up_blocks[1][0].load_state_dict(pipe_unet.up_blocks[0].resnets[1].state_dict())\n",
        "    myUNet.up_blocks[2][0].load_state_dict(pipe_unet.up_blocks[0].resnets[2].state_dict())\n",
        "    myUNet.up_blocks[2][1].load_state_dict(pipe_unet.up_blocks[0].upsamplers[0].state_dict())\n",
        "    # % upblock 1\n",
        "    myUNet.up_blocks[3][0].load_state_dict(pipe_unet.up_blocks[1].resnets[0].state_dict())\n",
        "    myUNet.up_blocks[3][1].load_state_dict(pipe_unet.up_blocks[1].attentions[0].state_dict())\n",
        "    myUNet.up_blocks[4][0].load_state_dict(pipe_unet.up_blocks[1].resnets[1].state_dict())\n",
        "    myUNet.up_blocks[4][1].load_state_dict(pipe_unet.up_blocks[1].attentions[1].state_dict())\n",
        "    myUNet.up_blocks[5][0].load_state_dict(pipe_unet.up_blocks[1].resnets[2].state_dict())\n",
        "    myUNet.up_blocks[5][1].load_state_dict(pipe_unet.up_blocks[1].attentions[2].state_dict())\n",
        "    myUNet.up_blocks[5][2].load_state_dict(pipe_unet.up_blocks[1].upsamplers[0].state_dict())\n",
        "    # % upblock 2\n",
        "    myUNet.up_blocks[6][0].load_state_dict(pipe_unet.up_blocks[2].resnets[0].state_dict())\n",
        "    myUNet.up_blocks[6][1].load_state_dict(pipe_unet.up_blocks[2].attentions[0].state_dict())\n",
        "    myUNet.up_blocks[7][0].load_state_dict(pipe_unet.up_blocks[2].resnets[1].state_dict())\n",
        "    myUNet.up_blocks[7][1].load_state_dict(pipe_unet.up_blocks[2].attentions[1].state_dict())\n",
        "    myUNet.up_blocks[8][0].load_state_dict(pipe_unet.up_blocks[2].resnets[2].state_dict())\n",
        "    myUNet.up_blocks[8][1].load_state_dict(pipe_unet.up_blocks[2].attentions[2].state_dict())\n",
        "    myUNet.up_blocks[8][2].load_state_dict(pipe_unet.up_blocks[2].upsamplers[0].state_dict())\n",
        "    # % upblock 3\n",
        "    myUNet.up_blocks[9][0].load_state_dict(pipe_unet.up_blocks[3].resnets[0].state_dict())\n",
        "    myUNet.up_blocks[9][1].load_state_dict(pipe_unet.up_blocks[3].attentions[0].state_dict())\n",
        "    myUNet.up_blocks[10][0].load_state_dict(pipe_unet.up_blocks[3].resnets[1].state_dict())\n",
        "    myUNet.up_blocks[10][1].load_state_dict(pipe_unet.up_blocks[3].attentions[1].state_dict())\n",
        "    myUNet.up_blocks[11][0].load_state_dict(pipe_unet.up_blocks[3].resnets[2].state_dict())\n",
        "    myUNet.up_blocks[11][1].load_state_dict(pipe_unet.up_blocks[3].attentions[2].state_dict())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "G_h_6tcQsOOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myunet = UNet_SD()\n",
        "original_unet = pipe.unet.cpu()\n",
        "load_pipe_into_our_UNet(myunet, original_unet)"
      ],
      "metadata": {
        "id": "voTf2vo6sm1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.unet = myunet.cuda()"
      ],
      "metadata": {
        "id": "BwFZstOIspxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A ballerina riding a Harley Motorcycle, CG Art\"\n",
        "with autocast(\"cuda\"):\n",
        "    image = pipe(prompt)[\"sample\"][0]\n",
        "\n",
        "image.save(\"astronaut_rides_horse.png\")"
      ],
      "metadata": {
        "id": "V1dpo2ortQqS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}